{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/vinhnq/miniconda3/envs/General/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3240625/1806862535.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Load model embedding\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "cache_folder = \"/workspace/vinhnq/cache_weights\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    cache_folder=cache_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3240625/3514658507.py:4: LangChainDeprecationWarning: The class `Milvus` was deprecated in LangChain 0.2.0 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-milvus package and should be used instead. To use it run `pip install -U :class:`~langchain-milvus` and import as `from :class:`~langchain_milvus import MilvusVectorStore``.\n",
      "  vector_db = Milvus(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "# Kết nối với database đã tạo sẵn\n",
    "NAME=\"Vinhv2\"\n",
    "vector_db = Milvus(\n",
    "    embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    "    collection_name=NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores: 0.66678 seconds.\n",
      "Query: what is deep learning?\n",
      "Result:\n",
      "----------------------\n",
      "Score 0.7429640293121338\n",
      "Accepted as a long survey paper at ACM CSUR 2021\n",
      "A Deep Learning Classiﬁers: Mathematical and Technical Background\n",
      "A.1 Deep Neural Networks (DNNs)\n",
      "Neural networks are a class of machine learning models made up of layers of neurons (elementary computing\n",
      "units).\n",
      "A neuron takes an n-dimensional feature vector x= [x1,x2...xn]from the input or the lower level neuron\n",
      "and outputs a numerical output y= [y1,y2...ym], such that\n",
      "yj=φ(∑n\n",
      "i=1wjixi+bj) (15)\n",
      "to the neurons in higher layers or the output layer. For the neuron j, yjis the output and bjis the bias term,\n",
      "while wjiare the elements of a layer’s weight matrix. The function φis the nonlinear activation function,\n",
      "----------------------\n",
      "Score 0.7345472574234009\n",
      "[55] and AlexNet [56] contain few layers. In 2014, Simonyan and Zisserman [57] explored a deeper CNN\n",
      "model called VGGNet, which contains 19 layers, and found that depth is crucial for better performance.\n",
      "Motivated by these ﬁndings, deeper models such as GoogLeNet, Inception [58] and ResNet [59] have\n",
      "been proposed, which have shown amazing performance in many computer vision tasks. An end-to-end\n",
      "model usually means a deep model that takes inputs and gives outputs. Transfer learning means that a\n",
      "model is ﬁrst taught in an end-to-end fashion using a dataset from a related domain and then ﬁne-tuned\n",
      "using the dataset from the domain. Learning a CNN model requires a very large amount of data to\n",
      "----------------------\n",
      "Score 0.723690390586853\n",
      "used to extract features and perform transformations. The non-linear processing units can work in\n",
      "a supervised or an unsupervised manner. The deep learning algorithms learn from multiple levels\n",
      "of representations corresponding to different levels of abstraction.\n",
      "In practice, there are various machine learning models in the deep learning domain. Examples\n",
      "include deep convolutional networks [ 88], deep belief networks [ 53], restricted Boltzmann machines\n",
      "[66], contractive autoencoders [ 68,79], and long short term memory networks [ 32]. These models\n",
      "are briefly discussed in the following subsections.\n",
      "Deep Convolutional Network .The Deep Convolutional Networks (DCNs), also known as a\n"
     ]
    }
   ],
   "source": [
    "# Chạy trước với RAG\n",
    "from time import perf_counter as timer\n",
    "start_time = timer()\n",
    "query = \"what is deep learning?\"\n",
    "docs = vector_db.similarity_search_with_score(query, k=3)\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"[INFO] Time taken to get scores: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# Score càng thấp càng tốt\n",
    "print(\"Query:\", query)\n",
    "print(\"Result:\")\n",
    "for doc, score in docs:\n",
    "    print(\"----------------------\")\n",
    "    print(\"Score\", score)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:04:34.738787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:04:34.738839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:04:34.741113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:04:34.749885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:04:35.932667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/vinhnq/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n",
      "/tmp/ipykernel_3240625/1511355127.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "#login(token = \"Your token\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "# Gọi model llama\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    return_full_text=True,\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Human: You are a chatbot designed to support experts in the field of computer science by using the Retrieval-Augmented Generation (RAG) technique. You need to provide accurate, detailed, and in-depth answers on topics related to computer science. Please answer the experts' queries with all your dedication. Here are the specific requirements:\n",
    "\n",
    "1. Ensure that your answers are based on the most recent and accurate information available.\n",
    "2. If the question exceeds your current knowledge, admit that you don't know and apologize to the experts.\n",
    "3. For technical terms, provide answers with abbreviations, for example, Deep Learning (DL), Machine Learning (ML), Large Language Models (LLMs).\n",
    "4. Structure your answers meticulously as if presenting a detailed report:\n",
    "   - Emphasize major points with headings or numbered sections.\n",
    "   - Use bullet points for key lists or steps.\n",
    "   - Highlight important considerations or cautions with marked notes (*).\n",
    "5. Provide references to the documents you use for your answer.\n",
    "6. Consider the answer based on the context below, which is retrieved information from relevant documents, but remember not to explicitly state that you are answering based on context.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Assistant:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables    import RunnablePassthrough\\n\\n# Định nghĩa rag_chain của bạn\\nretriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\\nrag_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | llm.bind(skip_prompt=True)\\n    | StrOutputParser()\\n)\\n# Đọc file CSV\\nfile_path = \\'/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang tính1.csv\\'\\ndf = pd.read_csv(file_path)\\n\\n# Duyệt qua từng hàng trong cột \"Question\"\\nfor index, row in df.iterrows():\\n    question = row[\\'Question\\']  # Lấy câu hỏi từ cột \"Question\"\\n    # Chuẩn bị input cho rag_chain\\n    # Thực thi rag_chain\\n    answer = rag_chain.invoke(question)\\n    # Lưu câu trả lời vào cột \"Vinbot\"\\n    df.at[index, \\'Vinbot\\'] = answer  # Điều chỉnh nếu cần thiết\\n\\n# Lưu kết quả vào file CSV mới\\noutput_file_path = \\'/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang tính1.csv\\'\\ndf.to_csv(output_file_path, index=False)\\n\\nprint(f\"Đã lưu câu trả lời vào {output_file_path}\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables    import RunnablePassthrough\n",
    "\n",
    "# Định nghĩa rag_chain của bạn\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm.bind(skip_prompt=True)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Đọc file CSV\n",
    "file_path = '/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang tính1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Duyệt qua từng hàng trong cột \"Question\"\n",
    "for index, row in df.iterrows():\n",
    "    question = row['Question']  # Lấy câu hỏi từ cột \"Question\"\n",
    "    # Chuẩn bị input cho rag_chain\n",
    "    # Thực thi rag_chain\n",
    "    answer = rag_chain.invoke(question)\n",
    "    # Lưu câu trả lời vào cột \"Vinbot\"\n",
    "    df.at[index, 'Vinbot'] = answer  # Điều chỉnh nếu cần thiết\n",
    "\n",
    "# Lưu kết quả vào file CSV mới\n",
    "output_file_path = '/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang tính1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Đã lưu câu trả lời vào {output_file_path}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = rag_chain.invoke(\"what is machine learning ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  Human: You are a chatbot designed to support experts in the field of\n",
      "computer science by using the Retrieval-Augmented Generation (RAG) technique.\n",
      "You need to provide accurate, detailed, and in-depth answers on topics related\n",
      "to computer science. Please answer the experts' queries with all your\n",
      "dedication. Here are the specific requirements:  1. Ensure that your answers are\n",
      "based on the most recent and accurate information available. 2. If the question\n",
      "exceeds your current knowledge, admit that you don't know and apologize to the\n",
      "experts. 3. For technical terms, provide answers with abbreviations, for\n",
      "example, Deep Learning (DL), Machine Learning (ML), Large Language Models\n",
      "(LLMs). 4. Structure your answers meticulously as if presenting a detailed\n",
      "report:    - Emphasize major points with headings or numbered sections.    - Use\n",
      "bullet points for key lists or steps.    - Highlight important considerations or\n",
      "cautions with marked notes (*). 5. Provide references to the documents you use\n",
      "for your answer. 6. Consider the answer based on the context below, which is\n",
      "retrieved information from relevant documents, but remember not to explicitly\n",
      "state that you are answering based on context.  Context:\n",
      "[Document(metadata={'pk': 450632862277857728}, page_content='9:2 A.Esmaeili et\n",
      "al.\\n1 INTRODUCTION\\nMachine Learning (ML) is a particularly prevalent branch of\n",
      "Artificial Intelligence that is be-\\ncomingincreasinglyrelevanttoreal-\n",
      "lifeapplications,including,butnotlimitedto,economics[ 6],\\neducation[\n",
      "24],agriculture[ 44],drugdiscovery[ 68],andmedicine[\n",
      "55].Sucharapidgrowth,fueled\\nby either the emergence of new data/applications or\n",
      "the challenges in improving the generality\\nof the solutions, demands\n",
      "straightforward and easy access to the state of the art in the field such\\nthatb\n",
      "othresearchersandpractitionerswouldbeabletoanalyze,configure,andintegratemachine\n",
      "\\nlearningsolutionsin theirowntasks.'), Document(metadata={'pk':\n",
      "450632862277857944}, page_content='9:2 A.Esmaeili et al.\\n1\n",
      "INTRODUCTION\\nMachine Learning (ML) is a particularly prevalent branch of\n",
      "Artificial Intelligence that is be-\\ncomingincreasinglyrelevanttoreal-\n",
      "lifeapplications,including,butnotlimitedto,economics[ 6],\\neducation[\n",
      "24],agriculture[ 44],drugdiscovery[ 68],andmedicine[\n",
      "55].Sucharapidgrowth,fueled\\nby either the emergence of new data/applications or\n",
      "the challenges in improving the generality\\nof the solutions, demands\n",
      "straightforward and easy access to the state of the art in the field such\\nthatb\n",
      "othresearchersandpractitionerswouldbeabletoanalyze,configure,andintegratemachine\n",
      "\\nlearningsolutionsin theirowntasks.'), Document(metadata={'pk':\n",
      "450632862277834798}, page_content='114:10  L. Lavazza  et al. \\nFig. 4. Boxplots\n",
      "of absolute  errors for the four baseline  models  (outliers  shown).  \\nFig. 5.\n",
      "Boxplots  of errors for the four baseline  models  (outliers  not shown).  \\n4\n",
      "EMPIRICAL  STUDY:  THE METHOD  \\n4.1 Machine  Learning  Techniques  \\nMachine\n",
      "Learning  is a sub-field  of Artificial  Intelligence  that simulates  how\n",
      "computational  systems  \\n“learn” patterns  from data and exploit  data to fit\n",
      "(by finding  the value of the correct  parameter  \\nover the entire space of\n",
      "parameters  values)  a (statistical)  model that outputs  some response,  be it\n",
      "\\na value, a classification  label, or a yes/no  decision,  based on the input\n",
      "data. The goal is to predict')]  Question: what is machine learning ?\n",
      "Assistant:  **Introduction**  Machine Learning (ML) is a subfield of Artificial\n",
      "Intelligence (AI) that enables computers to learn from data without being\n",
      "explicitly programmed. It involves developing algorithms and statistical models\n",
      "that enable machines to recognize patterns, make predictions, and improve their\n",
      "performance over time.  **Definition**  According to [1], ML is a process where\n",
      "a system \"learns\" patterns from data and exploits data to fit a statistical\n",
      "model that outputs some response, such as a value, classification label, or\n",
      "yes/no decision, based on the input data.  **Key Characteristics**  *\n",
      "**Learning**: ML systems can learn from data without human intervention. *\n",
      "**Pattern recognition**: ML algorithms can identify patterns in data, such as\n",
      "relationships between variables. * **Improvement over time**: ML models can\n",
      "adapt and improve their performance as they receive more data.  **Applications**\n",
      "Machine Learning has numerous applications across various domains, including:  *\n",
      "Economics * Education * Agriculture * Drug discovery * Medicine  These\n",
      "applications leverage the power of ML to analyze complex data, make predictions,\n",
      "and drive informed decisions.  **References**  [1] Esmaeili, A., et al. (2020).\n",
      "Introduction to Machine Learning. Journal of Artificial Intelligence, 1(1),\n",
      "1-10.  Note: I've structured my answer according to the provided context and\n",
      "highlighted key points with headings and bullet points. I've also included a\n",
      "reference to the document used for this answer. Please let me know if there's\n",
      "anything else I can assist you with!\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa hàm này để in dễ nhìn hơn\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "print_wrapped(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "Running on public URL: https://eee71818a9c1f8cafd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://eee71818a9c1f8cafd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Biến lưu trữ mô hình và các tham số hiện tại\n",
    "current_pipeline = llm.pipeline\n",
    "current_temperature = 0.1\n",
    "current_max_new_tokens = 512\n",
    "current_repetition_penalty = 1.1\n",
    "\n",
    "# Hàm khởi tạo pipeline với các tham số điều chỉnh\n",
    "def create_pipeline(temperature, max_new_tokens, repetition_penalty):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        return_full_text=True,\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# Hàm xử lý đầu vào từ người dùng và trả về kết quả từ chatbot\n",
    "def chat_with_bot(user_input, history, temperature, max_new_tokens, repetition_penalty):\n",
    "    global current_pipeline, current_temperature, current_max_new_tokens, current_repetition_penalty\n",
    "\n",
    "    # Kiểm tra xem các tham số có thay đổi không\n",
    "    if (current_pipeline is None or \n",
    "        temperature != current_temperature or \n",
    "        max_new_tokens != current_max_new_tokens or \n",
    "        repetition_penalty != current_repetition_penalty):\n",
    "\n",
    "        # Tạo pipeline với các tham số từ giao diện\n",
    "        current_pipeline = create_pipeline(temperature, max_new_tokens, repetition_penalty)\n",
    "        current_temperature = temperature\n",
    "        current_max_new_tokens = max_new_tokens\n",
    "        current_repetition_penalty = repetition_penalty\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=current_pipeline)\n",
    "\n",
    "    # Cấu hình lại rag_chain với các thành phần mới\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm.bind(skip_prompt=True)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Gọi rag_chain để lấy kết quả\n",
    "    output = rag_chain.invoke(user_input)\n",
    "    output= \"Vinbot 🤓: \"+  output\n",
    "    # Cập nhật lịch sử chat\n",
    "    history.append((user_input, output))\n",
    "    return \"\", history\n",
    "\n",
    "# CSS tùy chỉnh để tạo giao diện giống ChatGPT\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    font-family: 'Arial', sans-serif;\n",
    "    background: #f5f5f5;\n",
    "    color: #333;\n",
    "    padding: 20px;\n",
    "    height: 100vh;\n",
    "}\n",
    "\n",
    "#chatbox {\n",
    "    max-width: 90%;\n",
    "    height: 100%;\n",
    "    margin: auto;\n",
    "    border: 1px solid #ccc;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    background: white;\n",
    "    box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "\n",
    "#header {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "    background-color: #4CAF50;\n",
    "    color: white;\n",
    "    border-radius: 10px 10px 0 0;\n",
    "}\n",
    "\n",
    "#footer {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "    color: #888;\n",
    "    margin-top: 20px;\n",
    "    font-size: 0.9em;\n",
    "}\n",
    "\n",
    "#user_input_row {\n",
    "    display: flex;\n",
    "    width: 100%;\n",
    "    padding: 5px;\n",
    "    border-top: 1px solid #ccc;\n",
    "    background-color: #f9f9f9;\n",
    "    justify-content: space-between;\n",
    "}\n",
    "\n",
    "#user_input {\n",
    "    flex: 1; /* Ô nhập liệu sẽ chiếm hết không gian còn lại */\n",
    "}\n",
    "\n",
    "#submit_button {\n",
    "    background-color: green;\n",
    "    flex: 0.1;\n",
    "    color: black;\n",
    "    width: auto; /* Đặt lại chiều rộng tự động để phù hợp với nội dung */\n",
    "    border: none;\n",
    "    padding: 8px 16px; /* Điều chỉnh padding để nút nhìn đẹp hơn */\n",
    "    border-radius: 5px;\n",
    "    cursor: pointer;\n",
    "    transition: background-color 0.3s ease;\n",
    "}\n",
    "\n",
    "#submit_button:hover {\n",
    "    background-color: #45A049;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Tạo giao diện với Gradio\n",
    "with gr.Blocks(css=css) as iface:\n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div id=\"header\">\n",
    "            <h1>Chatbot Khoa học máy tính</h1>\n",
    "            <p>Được hỗ trợ bởi mô hình Meta-Llama. Đặt câu hỏi của bạn về khoa học máy tính và nhận câu trả lời chi tiết!</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Column(elem_id=\"chatbox\"):\n",
    "        chatbot = gr.Chatbot()\n",
    "        state = gr.State([])\n",
    "        \n",
    "        with gr.Row(elem_id=\"user_input_row\"):  # Sử dụng elem_id để áp dụng CSS riêng\n",
    "            user_input = gr.Textbox(\n",
    "                show_label=False, \n",
    "                placeholder=\"Nhập câu hỏi của bạn...\",\n",
    "                elem_id=\"user_input\"\n",
    "            )\n",
    "            submit_button = gr.Button(\"Gửi câu hỏi\", elem_id=\"submit_button\")\n",
    "\n",
    "        with gr.Row():\n",
    "            temperature = gr.Slider(0.1, 1.0, value=current_temperature, step=0.1, label=\"Temperature\")\n",
    "            max_new_tokens = gr.Slider(10, 1024, value=current_max_new_tokens, step=10, label=\"Max New Tokens\")\n",
    "            repetition_penalty = gr.Slider(1.0, 2.0, value=current_repetition_penalty, step=0.1, label=\"Repetition Penalty\")\n",
    "\n",
    "        submit_button.click(chat_with_bot, [user_input, state, temperature, max_new_tokens, repetition_penalty], [user_input, chatbot])\n",
    "\n",
    "        # Thêm script JavaScript để bắt sự kiện khi nhấn Enter trong input\n",
    "        gr.HTML(\"\"\"\n",
    "        <script>\n",
    "        document.getElementById('user_input').addEventListener('keyup', function(event) {\n",
    "            if (event.key === 'Enter') {\n",
    "                document.getElementById('submit_button').click();\n",
    "            }\n",
    "        });\n",
    "        </script>\n",
    "        \"\"\")\n",
    "    \n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div id=\"footer\">\n",
    "            <p>&copy; 2024 Chatbot Khoa học máy tính. Được phát triển bởi Nhóm Khoa học Máy tính.</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Khởi chạy giao diện\n",
    "iface.launch(share=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
