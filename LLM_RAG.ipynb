{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/vinhnq/miniconda3/envs/General/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3523433/1806862535.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-05-22 14:35:22.787739: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-22 14:35:22.787885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-22 14:35:23.044874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-22 14:35:23.544796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-22 14:35:28.008963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Load model embedding\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "cache_folder = \"/workspace/vinhnq/cache_weights\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    cache_folder=cache_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3523433/3514658507.py:4: LangChainDeprecationWarning: The class `Milvus` was deprecated in LangChain 0.2.0 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-milvus package and should be used instead. To use it run `pip install -U :class:`~langchain-milvus` and import as `from :class:`~langchain_milvus import MilvusVectorStore``.\n",
      "  vector_db = Milvus(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "# K·∫øt n·ªëi v·ªõi database ƒë√£ t·∫°o s·∫µn\n",
    "NAME=\"Vin\"\n",
    "vector_db = Milvus(\n",
    "    embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    "    collection_name=NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores: 3.78969 seconds.\n",
      "Query: what is deep learning?\n",
      "Result:\n",
      "----------------------\n",
      "Score 0.7429640293121338\n",
      "Accepted as a long survey paper at ACM CSUR 2021\n",
      "A Deep Learning ClassiÔ¨Åers: Mathematical and Technical Background\n",
      "A.1 Deep Neural Networks (DNNs)\n",
      "Neural networks are a class of machine learning models made up of layers of neurons (elementary computing\n",
      "units).\n",
      "A neuron takes an n-dimensional feature vector x= [x1,x2...xn]from the input or the lower level neuron\n",
      "and outputs a numerical output y= [y1,y2...ym], such that\n",
      "yj=œÜ(‚àën\n",
      "i=1wjixi+bj) (15)\n",
      "to the neurons in higher layers or the output layer. For the neuron j, yjis the output and bjis the bias term,\n",
      "while wjiare the elements of a layer‚Äôs weight matrix. The function œÜis the nonlinear activation function,\n",
      "----------------------\n",
      "Score 0.7345472574234009\n",
      "[55] and AlexNet [56] contain few layers. In 2014, Simonyan and Zisserman [57] explored a deeper CNN\n",
      "model called VGGNet, which contains 19 layers, and found that depth is crucial for better performance.\n",
      "Motivated by these Ô¨Åndings, deeper models such as GoogLeNet, Inception [58] and ResNet [59] have\n",
      "been proposed, which have shown amazing performance in many computer vision tasks. An end-to-end\n",
      "model usually means a deep model that takes inputs and gives outputs. Transfer learning means that a\n",
      "model is Ô¨Årst taught in an end-to-end fashion using a dataset from a related domain and then Ô¨Åne-tuned\n",
      "using the dataset from the domain. Learning a CNN model requires a very large amount of data to\n",
      "----------------------\n",
      "Score 0.723690390586853\n",
      "used to extract features and perform transformations. The non-linear processing units can work in\n",
      "a supervised or an unsupervised manner. The deep learning algorithms learn from multiple levels\n",
      "of representations corresponding to different levels of abstraction.\n",
      "In practice, there are various machine learning models in the deep learning domain. Examples\n",
      "include deep convolutional networks [ 88], deep belief networks [ 53], restricted Boltzmann machines\n",
      "[66], contractive autoencoders [ 68,79], and long short term memory networks [ 32]. These models\n",
      "are briefly discussed in the following subsections.\n",
      "Deep Convolutional Network .The Deep Convolutional Networks (DCNs), also known as a\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y tr∆∞·ªõc v·ªõi RAG\n",
    "from time import perf_counter as timer\n",
    "start_time = timer()\n",
    "query = \"what is deep learning?\"\n",
    "docs = vector_db.similarity_search_with_score(query, k=3)\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"[INFO] Time taken to get scores: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# Score c√†ng th·∫•p c√†ng t·ªët\n",
    "print(\"Query:\", query)\n",
    "print(\"Result:\")\n",
    "for doc, score in docs:\n",
    "    print(\"----------------------\")\n",
    "    print(\"Score\", score)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/vinhnq/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:13<00:00,  3.29s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3523433/1511355127.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "login(token = \"\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "# G·ªçi model llama\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    return_full_text=True,\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Human: You are a chatbot designed to support experts in the field of computer science by using the Retrieval-Augmented Generation (RAG) technique. You need to provide accurate, detailed, and in-depth answers on topics related to computer science. Please answer the experts' queries with all your dedication. Here are the specific requirements:\n",
    "\n",
    "1. Ensure that your answers are based on the most recent and accurate information available.\n",
    "2. If the question exceeds your current knowledge, admit that you don't know and apologize to the experts.\n",
    "3. For technical terms, provide answers with abbreviations, for example, Deep Learning (DL), Machine Learning (ML), Large Language Models (LLMs).\n",
    "4. Structure your answers meticulously as if presenting a detailed report:\n",
    "   - Emphasize major points with headings or numbered sections.\n",
    "   - Use bullet points for key lists or steps.\n",
    "   - Highlight important considerations or cautions with marked notes (*).\n",
    "5. Provide references to the documents you use for your answer.\n",
    "6. Consider the answer based on the context below, which is retrieved information from relevant documents, but remember not to explicitly state that you are answering based on context.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Assistant:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = rag_chain.invoke(\"what is machine learning ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  Human: You are a chatbot designed to support experts in the field of\n",
      "computer science by using the Retrieval-Augmented Generation (RAG) technique.\n",
      "You need to provide accurate, detailed, and in-depth answers on topics related\n",
      "to computer science. Please answer the experts' queries with all your\n",
      "dedication. Here are the specific requirements:  1. Ensure that your answers are\n",
      "based on the most recent and accurate information available. 2. If the question\n",
      "exceeds your current knowledge, admit that you don't know and apologize to the\n",
      "experts. 3. For technical terms, provide answers with abbreviations, for\n",
      "example, Deep Learning (DL), Machine Learning (ML), Large Language Models\n",
      "(LLMs). 4. Structure your answers meticulously as if presenting a detailed\n",
      "report:    - Emphasize major points with headings or numbered sections.    - Use\n",
      "bullet points for key lists or steps.    - Highlight important considerations or\n",
      "cautions with marked notes (*). 5. Provide references to the documents you use\n",
      "for your answer. 6. Consider the answer based on the context below, which is\n",
      "retrieved information from relevant documents, but remember not to explicitly\n",
      "state that you are answering based on context. Context:\n",
      "[Document(metadata={'pk': 450632862277857728}, page_content='9:2 A.Esmaeili et\n",
      "al.\\n1 INTRODUCTION\\nMachine Learning (ML) is a particularly prevalent branch of\n",
      "Artificial Intelligence that is be-\\ncomingincreasinglyrelevanttoreal-\n",
      "lifeapplications,including,butnotlimitedto,economics[ 6],\\neducation[\n",
      "24],agriculture[ 44],drugdiscovery[ 68],andmedicine[\n",
      "55].Sucharapidgrowth,fueled\\nby either the emergence of new data/applications or\n",
      "the challenges in improving the generality\\nof the solutions, demands\n",
      "straightforward and easy access to the state of the art in the field such\\nthatb\n",
      "othresearchersandpractitionerswouldbeabletoanalyze,configure,andintegratemachine\n",
      "\\nlearningsolutionsin theirowntasks.'), Document(metadata={'pk':\n",
      "450632862277857944}, page_content='9:2 A.Esmaeili et al.\\n1\n",
      "INTRODUCTION\\nMachine Learning (ML) is a particularly prevalent branch of\n",
      "Artificial Intelligence that is be-\\ncomingincreasinglyrelevanttoreal-\n",
      "lifeapplications,including,butnotlimitedto,economics[ 6],\\neducation[\n",
      "24],agriculture[ 44],drugdiscovery[ 68],andmedicine[\n",
      "55].Sucharapidgrowth,fueled\\nby either the emergence of new data/applications or\n",
      "the challenges in improving the generality\\nof the solutions, demands\n",
      "straightforward and easy access to the state of the art in the field such\\nthatb\n",
      "othresearchersandpractitionerswouldbeabletoanalyze,configure,andintegratemachine\n",
      "\\nlearningsolutionsin theirowntasks.'), Document(metadata={'pk':\n",
      "450632862277834798}, page_content='114:10  L. Lavazza  et al. \\nFig. 4. Boxplots\n",
      "of absolute  errors for the four baseline  models  (outliers  shown).  \\nFig. 5.\n",
      "Boxplots  of errors for the four baseline  models  (outliers  not shown).  \\n4\n",
      "EMPIRICAL  STUDY:  THE METHOD  \\n4.1 Machine  Learning  Techniques  \\nMachine\n",
      "Learning  is a sub-field  of Artificial  Intelligence  that simulates  how\n",
      "computational  systems  \\n‚Äúlearn‚Äù patterns  from data and exploit  data to fit\n",
      "(by finding  the value of the correct  parameter  \\nover the entire space of\n",
      "parameters  values)  a (statistical)  model that outputs  some response,  be it\n",
      "\\na value, a classification  label, or a yes/no  decision,  based on the input\n",
      "data. The goal is to predict')]  Question: what is machine learning ?\n",
      "Assistant:  **Introduction**  Machine Learning (ML) is a subfield of Artificial\n",
      "Intelligence (AI) that enables computers to learn from data without being\n",
      "explicitly programmed. It involves developing algorithms and statistical models\n",
      "that enable machines to recognize patterns, make predictions, and improve their\n",
      "performance over time.  **Definition**  According to [1], ML is a process where\n",
      "a system \"learns\" patterns from data and exploits data to fit a statistical\n",
      "model that outputs a response based on the input data. This response can be a\n",
      "value, a classification label, or a yes/no decision.  **Applications**  Machine\n",
      "Learning has numerous applications across various domains, including economics\n",
      "[6], education [24], agriculture [44], drug discovery [68], and medicine [55].\n",
      "Its rapid growth is fueled by the emergence of new data and applications, as\n",
      "well as the challenges in improving the generality of its solutions.  **Key\n",
      "Characteristics**  Some key characteristics of Machine Learning include:  *\n",
      "**Pattern recognition**: ML algorithms identify patterns in data to make\n",
      "predictions or classify new instances. * **Data-driven**: ML relies heavily on\n",
      "data to train and improve its models. * **Automation**: ML automates many tasks,\n",
      "reducing the need for human intervention. * **Continuous improvement**: ML\n",
      "models can adapt and improve over time as more data becomes available.\n",
      "**References**  [1] Esmaeili, A., et al. (2020). Introduction to Machine\n",
      "Learning. Journal of Artificial Intelligence Research, 123(1-2), 1-15.  [6]\n",
      "Economics application reference  [24] Education application reference  [44]\n",
      "Agriculture application reference  [68] Drug discovery application reference\n",
      "[55] Medicine application reference  Please let me know if this meets your\n",
      "expectations!\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a h√†m n√†y ƒë·ªÉ in d·ªÖ nh√¨n h∆°n\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "print_wrapped(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://c3beff65e588baa9dd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c3beff65e588baa9dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Bi·∫øn l∆∞u tr·ªØ m√¥ h√¨nh v√† c√°c tham s·ªë hi·ªán t·∫°i\n",
    "current_pipeline = llm.pipeline\n",
    "current_temperature = 0.1\n",
    "current_max_new_tokens = 512\n",
    "current_repetition_penalty = 1.1\n",
    "\n",
    "# H√†m kh·ªüi t·∫°o pipeline v·ªõi c√°c tham s·ªë ƒëi·ªÅu ch·ªânh\n",
    "def create_pipeline(temperature, max_new_tokens, repetition_penalty):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        return_full_text=True,\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# H√†m x·ª≠ l√Ω ƒë·∫ßu v√†o t·ª´ ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ª´ chatbot\n",
    "def chat_with_bot(user_input, history, temperature, max_new_tokens, repetition_penalty):\n",
    "    global current_pipeline, current_temperature, current_max_new_tokens, current_repetition_penalty\n",
    "\n",
    "    # Ki·ªÉm tra xem c√°c tham s·ªë c√≥ thay ƒë·ªïi kh√¥ng\n",
    "    if (current_pipeline is None or \n",
    "        temperature != current_temperature or \n",
    "        max_new_tokens != current_max_new_tokens or \n",
    "        repetition_penalty != current_repetition_penalty):\n",
    "\n",
    "        # T·∫°o pipeline v·ªõi c√°c tham s·ªë t·ª´ giao di·ªán\n",
    "        current_pipeline = create_pipeline(temperature, max_new_tokens, repetition_penalty)\n",
    "        current_temperature = temperature\n",
    "        current_max_new_tokens = max_new_tokens\n",
    "        current_repetition_penalty = repetition_penalty\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=current_pipeline)\n",
    "\n",
    "    # C·∫•u h√¨nh l·∫°i rag_chain v·ªõi c√°c th√†nh ph·∫ßn m·ªõi\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm.bind(skip_prompt=True)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # G·ªçi rag_chain ƒë·ªÉ l·∫•y k·∫øt qu·∫£\n",
    "    output = rag_chain.invoke(user_input)\n",
    "    output= \"Vinbot ü§ì: \"+  output\n",
    "    # C·∫≠p nh·∫≠t l·ªãch s·ª≠ chat\n",
    "    history.append((user_input, output))\n",
    "    return \"\", history\n",
    "\n",
    "# CSS t√πy ch·ªânh ƒë·ªÉ t·∫°o giao di·ªán gi·ªëng ChatGPT\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    font-family: 'Arial', sans-serif;\n",
    "    background: #f5f5f5;\n",
    "    color: #333;\n",
    "    padding: 20px;\n",
    "    height: 100vh;\n",
    "}\n",
    "\n",
    "#chatbox {\n",
    "    max-width: 90%;\n",
    "    height: 100%;\n",
    "    margin: auto;\n",
    "    border: 1px solid #ccc;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    background: white;\n",
    "    box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "\n",
    "#header {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "    background-color: #4CAF50;\n",
    "    color: white;\n",
    "    border-radius: 10px 10px 0 0;\n",
    "}\n",
    "\n",
    "#footer {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "    color: #888;\n",
    "    margin-top: 20px;\n",
    "    font-size: 0.9em;\n",
    "}\n",
    "\n",
    "#user_input_row {\n",
    "    display: flex;\n",
    "    width: 100%;\n",
    "    padding: 5px;\n",
    "    border-top: 1px solid #ccc;\n",
    "    background-color: #f9f9f9;\n",
    "    justify-content: space-between;\n",
    "}\n",
    "\n",
    "#user_input {\n",
    "    flex: 1; /* √î nh·∫≠p li·ªáu s·∫Ω chi·∫øm h·∫øt kh√¥ng gian c√≤n l·∫°i */\n",
    "}\n",
    "\n",
    "#submit_button {\n",
    "    background-color: green;\n",
    "    flex: 0.1;\n",
    "    color: black;\n",
    "    width: auto; /* ƒê·∫∑t l·∫°i chi·ªÅu r·ªông t·ª± ƒë·ªông ƒë·ªÉ ph√π h·ª£p v·ªõi n·ªôi dung */\n",
    "    border: none;\n",
    "    padding: 8px 16px; /* ƒêi·ªÅu ch·ªânh padding ƒë·ªÉ n√∫t nh√¨n ƒë·∫πp h∆°n */\n",
    "    border-radius: 5px;\n",
    "    cursor: pointer;\n",
    "    transition: background-color 0.3s ease;\n",
    "}\n",
    "\n",
    "#submit_button:hover {\n",
    "    background-color: #45A049;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# T·∫°o giao di·ªán v·ªõi Gradio\n",
    "with gr.Blocks(css=css) as iface:\n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div id=\"header\">\n",
    "            <h1>Chatbot Khoa h·ªçc m√°y t√≠nh</h1>\n",
    "            <p>ƒê∆∞·ª£c h·ªó tr·ª£ b·ªüi m√¥ h√¨nh Meta-Llama. ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ khoa h·ªçc m√°y t√≠nh v√† nh·∫≠n c√¢u tr·∫£ l·ªùi chi ti·∫øt!</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Column(elem_id=\"chatbox\"):\n",
    "        chatbot = gr.Chatbot()\n",
    "        state = gr.State([])\n",
    "        \n",
    "        with gr.Row(elem_id=\"user_input_row\"):  # S·ª≠ d·ª•ng elem_id ƒë·ªÉ √°p d·ª•ng CSS ri√™ng\n",
    "            user_input = gr.Textbox(\n",
    "                show_label=False, \n",
    "                placeholder=\"Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...\",\n",
    "                elem_id=\"user_input\"\n",
    "            )\n",
    "            submit_button = gr.Button(\"G·ª≠i c√¢u h·ªèi\", elem_id=\"submit_button\")\n",
    "\n",
    "        with gr.Row():\n",
    "            temperature = gr.Slider(0.1, 1.0, value=current_temperature, step=0.1, label=\"Temperature\")\n",
    "            max_new_tokens = gr.Slider(10, 1024, value=current_max_new_tokens, step=10, label=\"Max New Tokens\")\n",
    "            repetition_penalty = gr.Slider(1.0, 2.0, value=current_repetition_penalty, step=0.1, label=\"Repetition Penalty\")\n",
    "\n",
    "        submit_button.click(chat_with_bot, [user_input, state, temperature, max_new_tokens, repetition_penalty], [user_input, chatbot])\n",
    "\n",
    "        # Th√™m script JavaScript ƒë·ªÉ b·∫Øt s·ª± ki·ªán khi nh·∫•n Enter trong input\n",
    "        gr.HTML(\"\"\"\n",
    "        <script>\n",
    "        document.getElementById('user_input').addEventListener('keyup', function(event) {\n",
    "            if (event.key === 'Enter') {\n",
    "                document.getElementById('submit_button').click();\n",
    "            }\n",
    "        });\n",
    "        </script>\n",
    "        \"\"\")\n",
    "    \n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div id=\"footer\">\n",
    "            <p>&copy; 2024 Chatbot Khoa h·ªçc m√°y t√≠nh. ƒê∆∞·ª£c ph√°t tri·ªÉn b·ªüi Nh√≥m Khoa h·ªçc M√°y t√≠nh.</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Kh·ªüi ch·∫°y giao di·ªán\n",
    "iface.launch(share=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
