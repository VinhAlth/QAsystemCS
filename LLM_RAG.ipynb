{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/vinhnq/miniconda3/envs/General/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3240625/1806862535.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Load model embedding\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "cache_folder = \"/workspace/vinhnq/cache_weights\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    cache_folder=cache_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3240625/3514658507.py:4: LangChainDeprecationWarning: The class `Milvus` was deprecated in LangChain 0.2.0 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-milvus package and should be used instead. To use it run `pip install -U :class:`~langchain-milvus` and import as `from :class:`~langchain_milvus import MilvusVectorStore``.\n",
      "  vector_db = Milvus(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "# K·∫øt n·ªëi v·ªõi database ƒë√£ t·∫°o s·∫µn\n",
    "NAME=\"Vinhv2\"\n",
    "vector_db = Milvus(\n",
    "    embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    "    collection_name=NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores: 0.66678 seconds.\n",
      "Query: what is deep learning?\n",
      "Result:\n",
      "----------------------\n",
      "Score 0.7429640293121338\n",
      "Accepted as a long survey paper at ACM CSUR 2021\n",
      "A Deep Learning ClassiÔ¨Åers: Mathematical and Technical Background\n",
      "A.1 Deep Neural Networks (DNNs)\n",
      "Neural networks are a class of machine learning models made up of layers of neurons (elementary computing\n",
      "units).\n",
      "A neuron takes an n-dimensional feature vector x= [x1,x2...xn]from the input or the lower level neuron\n",
      "and outputs a numerical output y= [y1,y2...ym], such that\n",
      "yj=œÜ(‚àën\n",
      "i=1wjixi+bj) (15)\n",
      "to the neurons in higher layers or the output layer. For the neuron j, yjis the output and bjis the bias term,\n",
      "while wjiare the elements of a layer‚Äôs weight matrix. The function œÜis the nonlinear activation function,\n",
      "----------------------\n",
      "Score 0.7345472574234009\n",
      "[55] and AlexNet [56] contain few layers. In 2014, Simonyan and Zisserman [57] explored a deeper CNN\n",
      "model called VGGNet, which contains 19 layers, and found that depth is crucial for better performance.\n",
      "Motivated by these Ô¨Åndings, deeper models such as GoogLeNet, Inception [58] and ResNet [59] have\n",
      "been proposed, which have shown amazing performance in many computer vision tasks. An end-to-end\n",
      "model usually means a deep model that takes inputs and gives outputs. Transfer learning means that a\n",
      "model is Ô¨Årst taught in an end-to-end fashion using a dataset from a related domain and then Ô¨Åne-tuned\n",
      "using the dataset from the domain. Learning a CNN model requires a very large amount of data to\n",
      "----------------------\n",
      "Score 0.723690390586853\n",
      "used to extract features and perform transformations. The non-linear processing units can work in\n",
      "a supervised or an unsupervised manner. The deep learning algorithms learn from multiple levels\n",
      "of representations corresponding to different levels of abstraction.\n",
      "In practice, there are various machine learning models in the deep learning domain. Examples\n",
      "include deep convolutional networks [ 88], deep belief networks [ 53], restricted Boltzmann machines\n",
      "[66], contractive autoencoders [ 68,79], and long short term memory networks [ 32]. These models\n",
      "are briefly discussed in the following subsections.\n",
      "Deep Convolutional Network .The Deep Convolutional Networks (DCNs), also known as a\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y tr∆∞·ªõc v·ªõi RAG\n",
    "from time import perf_counter as timer\n",
    "start_time = timer()\n",
    "query = \"what is deep learning?\"\n",
    "docs = vector_db.similarity_search_with_score(query, k=3)\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"[INFO] Time taken to get scores: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# Score c√†ng th·∫•p c√†ng t·ªët\n",
    "print(\"Query:\", query)\n",
    "print(\"Result:\")\n",
    "for doc, score in docs:\n",
    "    print(\"----------------------\")\n",
    "    print(\"Score\", score)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:04:34.738787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:04:34.738839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:04:34.741113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:04:34.749885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:04:35.932667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/vinhnq/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.11s/it]\n",
      "/tmp/ipykernel_3240625/1511355127.py:23: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "#login(token = \"Your token\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "# G·ªçi model llama\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    return_full_text=True,\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Human: You are a chatbot designed to support experts in the field of computer science by using the Retrieval-Augmented Generation (RAG) technique. You need to provide accurate, detailed, and in-depth answers on topics related to computer science. Please answer the experts' queries with all your dedication. Here are the specific requirements:\n",
    "\n",
    "1. Ensure that your answers are based on the most recent and accurate information available.\n",
    "2. If the question exceeds your current knowledge, admit that you don't know and apologize to the experts.\n",
    "3. For technical terms, provide answers with abbreviations, for example, Deep Learning (DL), Machine Learning (ML), Large Language Models (LLMs).\n",
    "4. Structure your answers meticulously as if presenting a detailed report:\n",
    "   - Emphasize major points with headings or numbered sections.\n",
    "   - Use bullet points for key lists or steps.\n",
    "   - Highlight important considerations or cautions with marked notes (*).\n",
    "5. Provide references to the documents you use for your answer.\n",
    "6. Consider the answer based on the context below, which is retrieved information from relevant documents, but remember not to explicitly state that you are answering based on context.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Assistant:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.runnables    import RunnablePassthrough\\n\\n# ƒê·ªãnh nghƒ©a rag_chain c·ªßa b·∫°n\\nretriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\\nrag_chain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\\n    | prompt\\n    | llm.bind(skip_prompt=True)\\n    | StrOutputParser()\\n)\\n# ƒê·ªçc file CSV\\nfile_path = \\'/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang t√≠nh1.csv\\'\\ndf = pd.read_csv(file_path)\\n\\n# Duy·ªát qua t·ª´ng h√†ng trong c·ªôt \"Question\"\\nfor index, row in df.iterrows():\\n    question = row[\\'Question\\']  # L·∫•y c√¢u h·ªèi t·ª´ c·ªôt \"Question\"\\n    # Chu·∫©n b·ªã input cho rag_chain\\n    # Th·ª±c thi rag_chain\\n    answer = rag_chain.invoke(question)\\n    # L∆∞u c√¢u tr·∫£ l·ªùi v√†o c·ªôt \"Vinbot\"\\n    df.at[index, \\'Vinbot\\'] = answer  # ƒêi·ªÅu ch·ªânh n·∫øu c·∫ßn thi·∫øt\\n\\n# L∆∞u k·∫øt qu·∫£ v√†o file CSV m·ªõi\\noutput_file_path = \\'/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang t√≠nh1.csv\\'\\ndf.to_csv(output_file_path, index=False)\\n\\nprint(f\"ƒê√£ l∆∞u c√¢u tr·∫£ l·ªùi v√†o {output_file_path}\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables    import RunnablePassthrough\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a rag_chain c·ªßa b·∫°n\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm.bind(skip_prompt=True)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# ƒê·ªçc file CSV\n",
    "file_path = '/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang t√≠nh1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Duy·ªát qua t·ª´ng h√†ng trong c·ªôt \"Question\"\n",
    "for index, row in df.iterrows():\n",
    "    question = row['Question']  # L·∫•y c√¢u h·ªèi t·ª´ c·ªôt \"Question\"\n",
    "    # Chu·∫©n b·ªã input cho rag_chain\n",
    "    # Th·ª±c thi rag_chain\n",
    "    answer = rag_chain.invoke(question)\n",
    "    # L∆∞u c√¢u tr·∫£ l·ªùi v√†o c·ªôt \"Vinbot\"\n",
    "    df.at[index, 'Vinbot'] = answer  # ƒêi·ªÅu ch·ªânh n·∫øu c·∫ßn thi·∫øt\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ v√†o file CSV m·ªõi\n",
    "output_file_path = '/workspace/vinhnq/RAG_bot/QA for Quora and Chatbot - Trang t√≠nh1.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"ƒê√£ l∆∞u c√¢u tr·∫£ l·ªùi v√†o {output_file_path}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = rag_chain.invoke(\"what is machine learning ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  Human: You are a chatbot designed to support experts in the field of\n",
      "computer science by using the Retrieval-Augmented Generation (RAG) technique.\n",
      "You need to provide accurate, detailed, and in-depth answers on topics related\n",
      "to computer science. Please answer the experts' queries with all your\n",
      "dedication. Here are the specific requirements:  1. Ensure that your answers are\n",
      "based on the most recent and accurate information available. 2. If the question\n",
      "exceeds your current knowledge, admit that you don't know and apologize to the\n",
      "experts. 3. For technical terms, provide answers with abbreviations, for\n",
      "example, Deep Learning (DL), Machine Learning (ML), Large Language Models\n",
      "(LLMs). 4. Structure your answers meticulously as if presenting a detailed\n",
      "report:    - Emphasize major points with headings or numbered sections.    - Use\n",
      "bullet points for key lists or steps.    - Highlight important considerations or\n",
      "cautions with marked notes (*). 5. Provide references to the documents you use\n",
      "for your answer. 6. Consider the answer based on the context below, which is\n",
      "retrieved information from relevant documents, but remember not to explicitly\n",
      "state that you are answering based on context.  Context:\n",
      "[Document(metadata={'pk': 450632862277857728}, page_content='9:2 A.Esmaeili et\n",
      "al.\\n1 INTRODUCTION\\nMachine Learning (ML) is a particularly prevalent branch of\n",
      "Artificial Intelligence that is be-\\ncomingincreasinglyrelevanttoreal-\n",
      "lifeapplications,including,butnotlimitedto,economics[ 6],\\neducation[\n",
      "24],agriculture[ 44],drugdiscovery[ 68],andmedicine[\n",
      "55].Sucharapidgrowth,fueled\\nby either the emergence of new data/applications or\n",
      "the challenges in improving the generality\\nof the solutions, demands\n",
      "straightforward and easy access to the state of the art in the field such\\nthatb\n",
      "othresearchersandpractitionerswouldbeabletoanalyze,configure,andintegratemachine\n",
      "\\nlearningsolutionsin theirowntasks.'), Document(metadata={'pk':\n",
      "450632862277857944}, page_content='9:2 A.Esmaeili et al.\\n1\n",
      "INTRODUCTION\\nMachine Learning (ML) is a particularly prevalent branch of\n",
      "Artificial Intelligence that is be-\\ncomingincreasinglyrelevanttoreal-\n",
      "lifeapplications,including,butnotlimitedto,economics[ 6],\\neducation[\n",
      "24],agriculture[ 44],drugdiscovery[ 68],andmedicine[\n",
      "55].Sucharapidgrowth,fueled\\nby either the emergence of new data/applications or\n",
      "the challenges in improving the generality\\nof the solutions, demands\n",
      "straightforward and easy access to the state of the art in the field such\\nthatb\n",
      "othresearchersandpractitionerswouldbeabletoanalyze,configure,andintegratemachine\n",
      "\\nlearningsolutionsin theirowntasks.'), Document(metadata={'pk':\n",
      "450632862277834798}, page_content='114:10  L. Lavazza  et al. \\nFig. 4. Boxplots\n",
      "of absolute  errors for the four baseline  models  (outliers  shown).  \\nFig. 5.\n",
      "Boxplots  of errors for the four baseline  models  (outliers  not shown).  \\n4\n",
      "EMPIRICAL  STUDY:  THE METHOD  \\n4.1 Machine  Learning  Techniques  \\nMachine\n",
      "Learning  is a sub-field  of Artificial  Intelligence  that simulates  how\n",
      "computational  systems  \\n‚Äúlearn‚Äù patterns  from data and exploit  data to fit\n",
      "(by finding  the value of the correct  parameter  \\nover the entire space of\n",
      "parameters  values)  a (statistical)  model that outputs  some response,  be it\n",
      "\\na value, a classification  label, or a yes/no  decision,  based on the input\n",
      "data. The goal is to predict')]  Question: what is machine learning ?\n",
      "Assistant:  **Introduction**  Machine Learning (ML) is a subfield of Artificial\n",
      "Intelligence (AI) that enables computers to learn from data without being\n",
      "explicitly programmed. It involves developing algorithms and statistical models\n",
      "that enable machines to recognize patterns, make predictions, and improve their\n",
      "performance over time.  **Definition**  According to [1], ML is a process where\n",
      "a system \"learns\" patterns from data and exploits data to fit a statistical\n",
      "model that outputs some response, such as a value, classification label, or\n",
      "yes/no decision, based on the input data.  **Key Characteristics**  *\n",
      "**Learning**: ML systems can learn from data without human intervention. *\n",
      "**Pattern recognition**: ML algorithms can identify patterns in data, such as\n",
      "relationships between variables. * **Improvement over time**: ML models can\n",
      "adapt and improve their performance as they receive more data.  **Applications**\n",
      "Machine Learning has numerous applications across various domains, including:  *\n",
      "Economics * Education * Agriculture * Drug discovery * Medicine  These\n",
      "applications leverage the power of ML to analyze complex data, make predictions,\n",
      "and drive informed decisions.  **References**  [1] Esmaeili, A., et al. (2020).\n",
      "Introduction to Machine Learning. Journal of Artificial Intelligence, 1(1),\n",
      "1-10.  Note: I've structured my answer according to the provided context and\n",
      "highlighted key points with headings and bullet points. I've also included a\n",
      "reference to the document used for this answer. Please let me know if there's\n",
      "anything else I can assist you with!\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a h√†m n√†y ƒë·ªÉ in d·ªÖ nh√¨n h∆°n\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "print_wrapped(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "Running on public URL: https://eee71818a9c1f8cafd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://eee71818a9c1f8cafd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Bi·∫øn l∆∞u tr·ªØ m√¥ h√¨nh v√† c√°c tham s·ªë hi·ªán t·∫°i\n",
    "current_pipeline = llm.pipeline\n",
    "current_temperature = 0.1\n",
    "current_max_new_tokens = 512\n",
    "current_repetition_penalty = 1.1\n",
    "\n",
    "# H√†m kh·ªüi t·∫°o pipeline v·ªõi c√°c tham s·ªë ƒëi·ªÅu ch·ªânh\n",
    "def create_pipeline(temperature, max_new_tokens, repetition_penalty):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        return_full_text=True,\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# H√†m x·ª≠ l√Ω ƒë·∫ßu v√†o t·ª´ ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ª´ chatbot\n",
    "def chat_with_bot(user_input, history, temperature, max_new_tokens, repetition_penalty):\n",
    "    global current_pipeline, current_temperature, current_max_new_tokens, current_repetition_penalty\n",
    "\n",
    "    # Ki·ªÉm tra xem c√°c tham s·ªë c√≥ thay ƒë·ªïi kh√¥ng\n",
    "    if (current_pipeline is None or \n",
    "        temperature != current_temperature or \n",
    "        max_new_tokens != current_max_new_tokens or \n",
    "        repetition_penalty != current_repetition_penalty):\n",
    "\n",
    "        # T·∫°o pipeline v·ªõi c√°c tham s·ªë t·ª´ giao di·ªán\n",
    "        current_pipeline = create_pipeline(temperature, max_new_tokens, repetition_penalty)\n",
    "        current_temperature = temperature\n",
    "        current_max_new_tokens = max_new_tokens\n",
    "        current_repetition_penalty = repetition_penalty\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=current_pipeline)\n",
    "\n",
    "    # C·∫•u h√¨nh l·∫°i rag_chain v·ªõi c√°c th√†nh ph·∫ßn m·ªõi\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm.bind(skip_prompt=True)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # G·ªçi rag_chain ƒë·ªÉ l·∫•y k·∫øt qu·∫£\n",
    "    output = rag_chain.invoke(user_input)\n",
    "    output= \"Vinbot ü§ì: \"+  output\n",
    "    # C·∫≠p nh·∫≠t l·ªãch s·ª≠ chat\n",
    "    history.append((user_input, output))\n",
    "    return \"\", history\n",
    "\n",
    "# CSS t√πy ch·ªânh ƒë·ªÉ t·∫°o giao di·ªán gi·ªëng ChatGPT\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    font-family: 'Arial', sans-serif;\n",
    "    background: #f5f5f5;\n",
    "    color: #333;\n",
    "    padding: 20px;\n",
    "    height: 100vh;\n",
    "}\n",
    "\n",
    "#chatbox {\n",
    "    max-width: 90%;\n",
    "    height: 100%;\n",
    "    margin: auto;\n",
    "    border: 1px solid #ccc;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    background: white;\n",
    "    box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "\n",
    "#header {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "    background-color: #4CAF50;\n",
    "    color: white;\n",
    "    border-radius: 10px 10px 0 0;\n",
    "}\n",
    "\n",
    "#footer {\n",
    "    text-align: center;\n",
    "    padding: 10px;\n",
    "    color: #888;\n",
    "    margin-top: 20px;\n",
    "    font-size: 0.9em;\n",
    "}\n",
    "\n",
    "#user_input_row {\n",
    "    display: flex;\n",
    "    width: 100%;\n",
    "    padding: 5px;\n",
    "    border-top: 1px solid #ccc;\n",
    "    background-color: #f9f9f9;\n",
    "    justify-content: space-between;\n",
    "}\n",
    "\n",
    "#user_input {\n",
    "    flex: 1; /* √î nh·∫≠p li·ªáu s·∫Ω chi·∫øm h·∫øt kh√¥ng gian c√≤n l·∫°i */\n",
    "}\n",
    "\n",
    "#submit_button {\n",
    "    background-color: green;\n",
    "    flex: 0.1;\n",
    "    color: black;\n",
    "    width: auto; /* ƒê·∫∑t l·∫°i chi·ªÅu r·ªông t·ª± ƒë·ªông ƒë·ªÉ ph√π h·ª£p v·ªõi n·ªôi dung */\n",
    "    border: none;\n",
    "    padding: 8px 16px; /* ƒêi·ªÅu ch·ªânh padding ƒë·ªÉ n√∫t nh√¨n ƒë·∫πp h∆°n */\n",
    "    border-radius: 5px;\n",
    "    cursor: pointer;\n",
    "    transition: background-color 0.3s ease;\n",
    "}\n",
    "\n",
    "#submit_button:hover {\n",
    "    background-color: #45A049;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# T·∫°o giao di·ªán v·ªõi Gradio\n",
    "with gr.Blocks(css=css) as iface:\n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div id=\"header\">\n",
    "            <h1>Chatbot Khoa h·ªçc m√°y t√≠nh</h1>\n",
    "            <p>ƒê∆∞·ª£c h·ªó tr·ª£ b·ªüi m√¥ h√¨nh Meta-Llama. ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ khoa h·ªçc m√°y t√≠nh v√† nh·∫≠n c√¢u tr·∫£ l·ªùi chi ti·∫øt!</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Column(elem_id=\"chatbox\"):\n",
    "        chatbot = gr.Chatbot()\n",
    "        state = gr.State([])\n",
    "        \n",
    "        with gr.Row(elem_id=\"user_input_row\"):  # S·ª≠ d·ª•ng elem_id ƒë·ªÉ √°p d·ª•ng CSS ri√™ng\n",
    "            user_input = gr.Textbox(\n",
    "                show_label=False, \n",
    "                placeholder=\"Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...\",\n",
    "                elem_id=\"user_input\"\n",
    "            )\n",
    "            submit_button = gr.Button(\"G·ª≠i c√¢u h·ªèi\", elem_id=\"submit_button\")\n",
    "\n",
    "        with gr.Row():\n",
    "            temperature = gr.Slider(0.1, 1.0, value=current_temperature, step=0.1, label=\"Temperature\")\n",
    "            max_new_tokens = gr.Slider(10, 1024, value=current_max_new_tokens, step=10, label=\"Max New Tokens\")\n",
    "            repetition_penalty = gr.Slider(1.0, 2.0, value=current_repetition_penalty, step=0.1, label=\"Repetition Penalty\")\n",
    "\n",
    "        submit_button.click(chat_with_bot, [user_input, state, temperature, max_new_tokens, repetition_penalty], [user_input, chatbot])\n",
    "\n",
    "        # Th√™m script JavaScript ƒë·ªÉ b·∫Øt s·ª± ki·ªán khi nh·∫•n Enter trong input\n",
    "        gr.HTML(\"\"\"\n",
    "        <script>\n",
    "        document.getElementById('user_input').addEventListener('keyup', function(event) {\n",
    "            if (event.key === 'Enter') {\n",
    "                document.getElementById('submit_button').click();\n",
    "            }\n",
    "        });\n",
    "        </script>\n",
    "        \"\"\")\n",
    "    \n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "        <div id=\"footer\">\n",
    "            <p>&copy; 2024 Chatbot Khoa h·ªçc m√°y t√≠nh. ƒê∆∞·ª£c ph√°t tri·ªÉn b·ªüi Nh√≥m Khoa h·ªçc M√°y t√≠nh.</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Kh·ªüi ch·∫°y giao di·ªán\n",
    "iface.launch(share=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
